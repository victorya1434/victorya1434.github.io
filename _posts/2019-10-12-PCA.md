---
layout: post
title: Principal component analysis
---
## Problem formulation
let the data $$X$$, 
\\[X = {x^{(1)}, x^{(2)}, ..., x^{(m)}}\\] 
\\[x^{(i)} = {x_1^{(i)}, x_2^{(i)}, ..., x_n^{(i)}}\\]
\\[X \in \mathbb{R}^{m\times{n}}\\]

the result C we expect after pca, 
\\[C = {c^{(1)}, c^{(2)}, ..., c^{(m)}}\\] 
\\[c^{(i)} = {c_1^{(i)}, c_2^{(i)}, ..., c_k^{(i)}}\\]
\\[C \in \mathbb{R}^{m\times{k}}\\]

## The intuition
we called pca a machine learning algorithm (an unsupervised one) because it learns from the data or more concretely, it 's based on the variance of the data in each dimension in order to remove those which have a small variance.<br/>
the purpose of pca is to find the new orthonormal basis so that the variances of the data in some dimension are small and we can skip it.

let the orthonormal basis $$U \in \mathbb{R}^{n\times{n}}$$, $$\widetilde{X}$$ is the data $$X$$ perform in $$U$$, $$D\in \mathbb{R}^{n\times{k}}$$ is the first k eigenvector of $$U$$ which have highest eigenvalues, $$EY$$ is the rest, we have:
\\[X^{T} = U\widetilde{X}^{T}\\]
\\[X^{T} = DC^{T} + EY\\]
\\[X^{T}\approx{DC^{T}} \\]

## The math
**note**: in this part, I 'll skip the optimization step and only show the result.

the final thing of pca we want to find out is the $$D$$, but the first thing we need to do is figure out how to generate the optimal code point $$c^{*}$$ for each input point $$x$$:
\\[ C^{*} = \arg\min_{C} ||X^{T} - DC^{T}||_2 ~~~~~~(1) \\]
and the result of $$(1)$$:
\\[C^{\*^{T}} = D^{T}X^{T}\\]
so the new $$X$$ we want to approximate is:
\\[X \approx DD^{T}X^{T}\\]
now let optimize $$D$$:
<!-- \\[ D^{*} = \arg \min_{D} \||X - DD^{T}X^{T}|\|_2 ~~~~~~(2) \\] -->
\\[ D^{\*} = \arg{\min_D} \||X - DD^{T}X^{T}|\|_2 ~~~~~~(2)\\]

after the optimization, $$D\in \mathbb{R}^{n\times{k}}$$ has their columns as k eigenvectors with highest eigenvalues of the matrix $$X^{T}X$$.

## The step (code)
```python
# my code on understanding pca
import numpy as np
import numpy.linalg as linalg
import matplotlib.pyplot as plt


m = 100  # num of sample
n = 2  # num of dim (vector size)
k = 1  # num of dim to take down
x = np.random.randn(m, n)

# compute the S matrix
mean = np.mean(x, axis=0)
x_ = x - mean  # subtract mean
S = np.matmul(x_.transpose(), x_)  # the S matrix

# compute eigen vector of S
eig = linalg.eig(S)

# the decode matrix D
D = eig[1][:k]  # get first k eigenvector (nx1) with highest eigenvalue
D = D.reshape(-1, k)  # change the shape to n x k

# the encode E
E = D.transpose()
z = np.matmul(E, x.transpose()).transpose()  # z (m x k), the result of pca

# visualize
# get two eigenbasis in new basis
d1 = eig[1][0]
d2 = eig[1][1]

# check d1 and d2 are orthonormal
print(linalg.norm(d1))  # 1.0
print(linalg.norm(d2))  # 1.0
print(np.dot(d1, d2))  # 0.0

# plot the x
t = 4
plt.scatter(x[:, 0], x[:, 1])

plt.arrow(0, 0, t*d1[0], t*d1[1], fc='red', ec='red')
plt.arrow(0, 0, t*d2[0], t*d2[1], fc='blue', ec='blue')

# plt.plot(np.linspace(-t, t, 1000), np.zeros_like(np.linspace(-t, t, 1000)))
# plt.plot(np.zeros_like(np.linspace(-t, t, 1000)), np.linspace(-t, t, 1000))
# plt.scatter(x=z[:, 0], y=np.zeros_like(z[:, 0]))
# plt.scatter(y=z[:, 0], x=np.zeros_like(z[:, 0]))
# plt.axis('equal')

plt.margins(x=0.5, y=0.5)
plt.show()
```
![_config.yml]({{ site.baseurl }}/images/pca.png)
<br/>
*Fig. 1: Two eigenvectors, the red one has the higher eigenvalue, project the data to this vector to get the new data with the dimension k=1*
